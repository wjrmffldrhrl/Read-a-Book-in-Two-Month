# 말뭉치의 전처리와 가공
# 문서 쪼개보기
사용자 지정 말뭉치 리더는 NLTK의 CorpusReader 객체를 상속하므로 표준 전처리 API도 구현하여 다음과 같은 메서드도 제공한다.
- `raw()`
  - 전처리 없이 원시 텍스트에 대한 액세스를 제공한다.
- `sents()`
  - 본문에 있는 개별 문장의 생성자다.
- `words()`
  - 텍스트를 개별 단어로 토큰화한다.  

우리의 텍스트에 머신러닝 기술을 사용해 모델을 적합화하려면 이러한 방법을 특징추출 과정의 일부로 포함시켜야 한다.  

## 핵심 내용 식별 및 추출
HTML 콘텐츠는 구조화된 상태에서 수많은 방식으로, 그리고 때로는 엉뚱한 방식으로 제작되고 렌더링될 수 있다. 이러한 예측 불가능성으로 인해 체계적인 방식으로 프로그램을 사용해서 원시 HTML 텍스트에서 데이터를 추출하기가 무척 어렵다.  

Readability-lxml 라이브러리는 웹에서 수집된 문서의 높은 변동성을 다루기에 정말 좋은 자원이다. Readability-lxml은 Arc90에 의한 자바스크립트 가독성 실험용 파이썬 래퍼다. 사파리나 크롬 같은 브라우저와 마찬가지로 읽기 모드를 제공하므로 Readability-lxml은 페이지의 내용에서 잡다한 것들을 제거해 텍스트만 남긴다.  

HTML 문서가 주어진 경우 Readability는 일련의 정규 표현식을 사용하여 탐색 모음, 광고, 페이지 스크립트 태그 및 CSS를 제거한 다음. 새로운 **문서 객체 모델(DOM)**트리를 작성하고, 원래 트리에서 텍스트를 추출하고, 새로 재구성된 트리 내에서 텍스트를 재구성한다. 다음 번에 나오는 HTMLCorpusReader를 확장한 예제에서 우리는 Unparseable 및 Document라는 두 가지 가독성 모듈을 가져와 전처리 작업흐름의 첫 번째 단계에서 원시 HTML 텍스트를 추출하고 정리한다.  

html 메서드는 각 파일을 반복하고 readability.Document 클래스의 summary 메서드를 사용해 스크립트가 아닌 모든 텍스트 콘텐트와 sty-listic 태그를 제거한다. 또한, 가장 자주 잘못 사용되는 태그(e.g. `<div>`와 `<br>`)를 수정하고, 원래의 HTML이 비교 불가능한 것으로 판명될 때만 예외를 던진다. 이러한 예외의 가장 큰 이유는 구문분석할 이유가 없는 빈 문서를 함수가 전달받은 경우다.  

## 문서를 단락별로 나누기
단락은 문서 구조의 단위로 기능하는 완전한 한 가지 관념을 담아 낸 부분을 말한다.  

PlaintextCorpusReader와 같은 일부 NLTK 말뭉치 리더는 paras() 메서드를 구현한다. paras() 메서드는 이중 줄바꿈으로 구분된 텍스트 블록으로 정의된 단락을 생성한다.  

그러나 여기서 우리가 다루려고 하는 텍스트는 일반적인 의미의 텍스트가 아니므로 우리는 HTML에서 단락을 추출하는 메서드를 만들어야 한다. 다행히도 우리의 html() 메서드는 HTML 문서의 구조를 유지한다. 즉, HTML 태그를 공식적으로 정의하는 `<p>`태그를 검색해 파라미터 내에서 나타나는 내용을 격리할 수 있다.  

parse() 메서드를 정의해 각 fileid를 반복하고, 각 HTML 문서를 BeautifulSoup 생성자에 전달함으로써, lxml이라는 HTML 구문분석기를 사용해 HTML을 구문분석하도록 지정한다. 그 결과로 나온 soup는 원래 HTML 태그의 요소를 사용해 탐색할 수 있는 중첩 트리 구조다. 각 문서 soup에 대해 미리 정의된 집합의 각 태그를 반복해 해당 태그 내에서 텍스트를 산출한다. 우리는 BeautifulSoup의 decompose 메서드를 호출해 메모리를 확보하기 위해 각 파일 작업을 마쳤을 때 트리를 파괴할 수 있다.

## 분할: 문장별로 나누기
단락을 문서 구조의 단위로 여긴다면 문장을 담론의 단위로 보는 것이 유용하다. 문서 내의 한 단락이 하나의 관념으로 구성되는 것과 마찬가지로 문장은 완전한 언어 구조를 포함하고 있다.  

`sents()`메서드는 내장된 NLTK sent_tokenize 메서드를 사용해 paras 메서드로 분리된 각 단락을 반복해 가며 분해 작업을 수행한다. 이면에서 보면 send_tokenize는 PunktsSentenceTokenizer를 사용하는데, 이 토크나이저는 문장의 시작과 끝을 알리는 단어와 구두점의 종류에 대해 변환 규칙을 배운 사전 훈련 모델이다. 

NLTK는 탐구할 만한 대안 문장에 대한 토큰화를 제공한다. 그럼에도 불구하고 영역 공간에 문장이 구분되는 방식에 특이한 것이 있다면 영역별 내용을 사용해 자체 토크나이저를 훈련하는게 좋다.

## 토큰화: 개별 토큰 식별
**토큰화(tokenization)**는 문자 시퀀스 내에서 의미 정보를 인코딩하는 언어의 구문론적 단위인 토큰에 도달하는 과정이다.

**문장 구획화(sentence demarcation)**와 마찬가지로 토큰화가 항상 쉽지는 않다. 토큰화에서 고려해야 할 점들은 아래와 같이 다양하다.  
- 토큰에서 구두점을 제거하고 싶다면 구두점 토큰을 만들어야 하는가? 
- 하이픈으로 연결해 만든 단어를 합성어로 봐야 하는가, 아니면 따로 뗴어서 보아야 하는가? 
- 우리는 축약형을 토큰 한 개 또는 두 개로 접근해야 하는가? 
- 그리고 토큰이 두 개라면 어디에서 나누어야 하는가?  

이러한 사항들에 따라 토크나이저를 선택할 수 있다. 

## 품사 태깅
**품사(part-of-speech)**는 문장의 내용 내에서 단어가 어떻게 작동하는지를 나타낸다. 품사의 예로는 동사(verbs), 명사(nouns), 전치사(prepositions), 형용사(adjectives)를 들 수 있다. 품사 태깅은 각 토큰에 적절한 태그를 붙이는 일을 말하며, 이럼으로써 단어의 정의와 문맥에서의 사용에 관한 정보를 모두 인코딩 할 수 있다.  

## 중간 말뭉치 분석론
HTMLCorpusReader에는 문서 분해를 수행하는데 필요한 모든 메서드가 있다. `sizes()` 메서드를 통해 시간이 지남에 따라 어떻게 말뭉치가 변화하는지를 대력적으로 알 수 있다. 이제 변화하는 범주, 어휘 및 복잡성에 대한 중간 말뭉치 분석을 수행할 수 있는 새로운 메서드인 `describe()`를 추가한다.  

`describe()`가 클럭을 시작하고 두 개의 빈도분포를 초기화한다. 첫 번째 카운트는 문서 하부 구조의 개수를 유지하고, 두 번째는 토큰을 포함한다. 각 단락, 문장, 단어의 개수를 세고 또한 어휘집에 독특한 토큰을 각기 저장한다. 그런 다음에는 우리는 말뭉치에 있는 파일과 범주읠 수를 계산하고, 말뭉치에 대한 통계적 요약을 담은 딕셔너리를 반환한다. 
> #### 통계적 요약
> 1. 파일 및 범주의 총 수
> 2. 단락, 문장 및 단어의 총 수 및 고유 용어의 수
> 3. 어휘적 다양성
> 4. 문서당 평균 단락 수
> 5. 단락당 평균 문장 수
> 6. 총 처리 시간

말뭉치는 수집, 전처리 및 압축을 통해 성장하므로 `describe()`를 통해 이러한 계량을 다시 계산해 시간 경과에 따른 변화를 확인할 수 있다. 이것은 애플리케이션의 ㅁ누제점을 진단하는데 도움이 된느 중요한 모니터링 기술이 될 수 있다. 머신러닝 모델은 문서당 어휘 다양성 및 단락 수와 같은 데이터의 특징이 일관성을 유지할 것으로 기대하며, 말뭉치가 변경되면 성능에 큰 영향을 미칠 수 있다. 따라서 `describe()`메서드를 사용해 모든 하위 스트림 벡터화 및 모델링을 다시 작성하기에 충분히 큰 말뭉치 변경 사항을 감시할 수 있다.


# 말뭉치 변환
이제 리더는 말뭉치의 원본 문서에 대해 내용 추출, 단락 구문, 문장 분할, 단어 토큰화, 품사 태깅이라는 단계들을 거쳐 전달할 수 있다. 그러나 이 전처리를 수행하는 과정은 자원이 많이 소모된다. 이에 HTMLCorpusReader를 감싸서 중간 변환된 말뭉치를 저장하기 위해 원시 말뭉치를 가공하는 Preprocessor 클래스와, 다운스트림 벡터화 및 분석을 위해 표준화된 방식으로 변환된 문서를 디스크에서 스트리밍할 수 있는 PickledCorpusReader를 추가한다.

## 중간 전처리 및 저장

우리는 말뭉치 리더를 감싸고 문서의 상태 저장 토큰화 및 품사 태깅을 관리할 새로운 클래스인 Preprocessor를 정의하는 일부터 한다. 객체들은 원시 말뭉치의 견로인 corpus, 후처리 말뭉치를 저장할 디렉터리 경로인 target으로 초기화된다. `fileids()` 메서드는 HTMLCorpus Reader 객체의 fileid에 대한 편리한 엑세스를 제공하고, `abspath()`는 각 원시 말뭉치 fileid에 대한 대상 fileid의 절대 경로를 반환한다.  

