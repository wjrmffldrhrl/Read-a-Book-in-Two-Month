# 사용자 정의 말뭉치 구축
모든 머신러닝 애플리케이션에서와 마찬가지로, 텍스트 분석 애플리케이션의 주요 도전 과제 또한 잡음 속에 신호라고 할 만한게 들어 있는지 여부를 알아내고, 그러한 신호가 어디에 들어 있는지를 식별해 내는 것이라고 할 수 있다. 이는 **특징분석(feature analysis)**과정을 거쳐 이루어진다. 즉, 우리는 텍스트 분석을 통해 어떠한 **특징(feature)**과 **속성(property)** 또는 **차원(dimensions)**이 의미와 기존 구조를 가장 잘 나타내는지를 결정하게 된다는 말이다.  

## 말뭉치란 무엇인가?
**말뭉치(corpora)**는 자연어가 들어 있는 관련 문서들의 **모음집(collection)**이다. 말뭉치는 문서들의 범주, 즉 개별 문서들의 범주로 분류할 수 있다. 말뭉치에 포함된 문서는 트윗에서 책에 이르기까지 크기가 다를 수 있지만, 텍스트 및 텍스트와 관련된 일련의 **관념(ideas)**이 들어 있다. 문서들에 담긴 내용을 각기 순서에 따라 단락으로 나눌 수 있다. **단락(paragraph)**이란 일반적으로 각기 하나의 관념을 표현하는 **담론 단위(units of discourse)**다. 단락은 **구문 단위(units of syntax)**인 문장으로 쪼갤 수 있다. 완전한 문장은 구조적으로 보면 특정한 표현식처럼 보일 수 있다. 문장은 단어와 구두점으로 구성되어 있는데, 어휘 단위가 일반적인 의미를 나타내지만 어휘 단어들을 조합하면 의미를 나타내기에 훨씬 더 유용하다. 마지막으로, 단어 자체는 음절, 음소, 접사 및 문자로 구성되며, 이와 같은 단위들은 단어로 조합될 때만 의미가 있게된다.

## 영역 특정 말뭉치
포괄적인 말뭉치로 자연어 모델을 테스트하는 것이 아주 일반적이다. 그러나 최상의 언어 모델은 종종 큰 제약을 받기도 하는데, 어런 면은 애플리케이션에 따라 달라진다.  

언어의 특정 분야나 특정 영역에서 훈련된 모델이 일반 언어로 훈련된 모델보다 더 잘 작동하는 이유는 영역마다 다른 언어를 사용하기 때문에, 즉 같은 언어를 쓸지라도 어휘나 약어 또는 일반적인 어구 등이 전문 분야별로 서로 다르므로 상대적으로 순수하게 해당 영역에 관한 문서 위주로 이루어진 말뭉치는 여러 영역의 문서가 들어 있는 말뭉치보다 더 잘 분석되고 모델링 될 수 있다. 모델을 좁은 영역에 집중하게 하면 예측 공간이 작아지고 더 구체적이어서 언어의 유연한 측면을 더 잘 처리할 수 있다. 잘 작동하는 언어 인식 데이터 제품을 생산하려면 **영역 특정 말뭉치(domain specific corpora)**를 획득하는 것이 필수적이다.  

## Baleen 수집 엔진
Baleen은 사용자 지정 말뭉치를 작성하기 위한 오픈소스 도구다.

RSS 피드의 OPML 파일이 제공되면 ballen은 해당 피드의 모든 게시물을 내려받아 MongoDB 저장소에 저장한 다음, 분석에 사용할 수 있는 텍스트 말뭉치를 내보낸다.  


## 말뭉치 데이터 관리  
우리가 첫 번째로 가정해야 할 점은 우리가 다루는 말뭉치들의 크기가 자명하지 않다는 점이다. 두 번째로 가정해야 할 점은 언어 데이터는 우리가 분석을 수행할 수 있을 만한 데이터 구조가 되도록 정제하고 처리되어야 하는 원천에서 나온다는 점이다. 첫 번째 가정을 따르자면 확장 가능한 컴퓨팅 방법론이 필요해지며, 두 번째 가정에 따르면 우리가 데이터에 돌이킬 수 없는 변형을 수행할 것임을 암시한다.  

데이터 제품은 처리 및 전처리 사이의 중간 데이터 관리 계층으로 **WORM(wirte-donce, read-many)** 저장장치를 사용한다. WORM 저장장치를 떄떄로 **데이터 레이크(data lake 데이터 호수)** 라고도 부르는데, 이 장치를 사용하면 원시 데이터를 확장 가능한 방식으로 저장해두고는 이 데이터를 스트림을 반복적으로 읽을 수 있기 때문에 대규모 계산 작업 시 필요한 기준을 충족할 수 있다. 또한, WORM 저장장치에 데이터를 보관하면 전처리된 데이터를 재처리하지 않고도 다시 분석할 수 있으므로 원시 데이터 형식에서 새로운 가설을 쉽게 탐색할 수 있다.

데이터는 어디에 저장해야 할까? 우리는 데이터 관리를 생각할 때 대개 데이터베이스를 먼저 생각한다. 데이터베이스는 언어 인식 데이터 제품을 구축하는 데 중요한 도구이며, 대부분 전체 텍스트 검색 기능과 다른 유형의 인덱싱을 제공한다. 그러나 대부분의 데이터베이스는 트랜잭션당 두 행만 검색하거나 갱신하도록 구성된다.  

이와 대조적으로, 텍스트 말뭉치에 대해 컴퓨팅 방식으로 액세스 하면 모든 문서를 완전히 읽게 되며, 문서에 대한 내부 갱신은 발생하지 않고, 개별 문서를 검색하거나 선택하지 않는다. 따라서 데이터베이스는 실제 이점 없이 계산에 오버헤드를 추가하는 경향이 있다.  

결론적으로 말뭉치들을 관계형 데이터베이스가 아닌 디스크에 저장하는편이 더 바람직하다.  

텍스트 데이터 관리의 경우, 최소한의 오버헤드를 문서를 스트리밍할 수 있는 NoSQL 문서 저장 데이터베이스에 저장하거나 각 문서를 디스크에 간단하게 작성하는 것이 최선의 선택이다.  

## 말뭉치 디스크 구조

텍스트 기반 말뭉치를 구성하고 관리하는 가장 간단하고 일반적인 방법은 개별 문서를 디스크의 파일 시스템에 저장하는 것이다. 말뭉치를 하위 디렉터리로 구성해 날짜와 같은 메타 정보로 분류하거나 의미 있게 분할할 수 있다. 각 문서를 자체 파일로 유지함으로써 말뭉치 리더는 서로 다른 뭇너의 부분집합을 신속하게 검색할 수 있으며, 각 과정은 서로 달느 문서 집합을 사용해 처리가 병렬화 될 수 있다.  

또한, 텍스트 형식은 가장 많이 압축할 수 있는 형식이기 때문에 디스크의 디렉터리 구조를 활용하는 Zip 파일을 이상적인 배포 형식과 저장 형식이 되게 한다. 마지막으로, 디스크에 저장된 라뭉치들은 일반적으로 정적이며 전체적으로 처리되므로 이전 절에서 설명한 WORM 저장장치의 요건을 충족해야 한다.  

문서가 다중 문서 파일로 집계되든 아니면 자체 파일로 저장되든 간에 말뭉치는 정리해야 할 많은 파일을 나타낸다. 시간의 흐름에 맞춰 말뭉치가 수집된다면 연, 월, 일별로 하위 폴더를 만들어 각 폴더에 문서를 두는 것이 의미 있는 조직 방식이 될 수 있다. 문서가 정서에 따라 긍정적이거나 부정적으로 분류된다면 각 유형의 문서는 함꼐 자신의 범주를 나타내는 하위 디렉터리로 그룹화 할 수 있다.

## 말뭉치 리더
대부분의 비자명 말뭉치(nontrivial corpora)에는 잠재적으로 기가바이트 크기인 텍스트 데이터가 포함된 수천 개의 문서가 포함되어 있다. 그런 다음에 문서에서 적재된 원시 텍스트 문자열을 미리 처리하고 분석에 적합한 표현으로 파싱해야 하는데, 이 방법을 따르면 데이터를 생성하거나 복제하는 데 필요한 작업 메모리 양이 늘어날 수 있다. 계산 관점에서 보면 이건 심각하게 고려해 보아야 할 문제다. 디스크에서 문서를 스트리밍하고 선택하는 방법이 없으면 텍스트 분석이 단일 시스템의 성능에 빠르게 고착되는 셈이 되어 흥미로운 모델을 생성할 수 없기 때문이다. 다행이 디스크에서 말뭉치에 대한 엑세스를 스트리밍 하는 도구가 NLTK 라이브러리로 잘 정비되어 있는데, 이 라이브러리는 CorpusReader 객체들을 통해 파이썬 내 말뭉치들을 노출한다.  

CorpusReader는 문서를 읽고 검색하고 스트리밍하고 필터링하는 프로그래밍 인터페이스이며, 말뭉치 내의 데이터에 액세스해야 하는 코드의 인코딩 및 전처리와 같은 데이터 가공 기술을 노출한다. CorpusReader는 말뭉치 파일들과 문서 이름을 검색하기 위한 서명 및 파일 인코딩을 포함하는 디렉터리에 루트 경로를 전달해 인스턴스화된다.  

CorpusReader라는 개념 그 자체만으로는 특별히 눈에 띄지 않을 수도 있깄지만, 수많은 문서를 다룰 때 이 객체의 인터페이스를 사용하면 프로그래머는 문서를 한 개 이상 메모리로 읽어들일 수도 있고, 불필요하게 문서를 읽거나 열지 않고도 말뭉치의 특정 위치를 앞뒤로 탐색해 분석 전문가에게 데이터를 스트리밍해 줄 수도 있고, 말뭉치에서 한 번에 특정 문서만을 선별하거나 선택할 수도 있다. 이러한 기술은 한 번에 메모리에 있는 단 몇 개의 문서에만 작업을 적용 하기 때문에 비자명 말뭉치에 대한 메모리 내 텍스트 분석을 가능하게 한다.  
